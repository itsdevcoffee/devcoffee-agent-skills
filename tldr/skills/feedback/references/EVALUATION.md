# TLDR Command Evaluation System

**Purpose:** Systematically evaluate and improve the `/tldr` command by collecting real-world usage samples and scoring them against quality criteria.

**Date Started:** 2026-02-06
**Status:** Active data collection (v1.0)

## Overview

This evaluation system tracks how well the `/tldr` command performs in real development scenarios. We collect samples of original Claude messages paired with their TLDR summaries, score them on multiple criteria, and identify patterns for improvement.

## How It Works

### 1. User Shares Sample

User pastes in chat:
```
Original: [Claude's original message]
TLDR: [The /tldr command output]
```

### 2. Claude Scores (0.0-10.0)

Evaluates on four criteria (2.5 points each):
- **Completeness:** Did it capture all key information?
- **Conciseness:** Did it stay focused and under 8 bullets?
- **Actionability:** Did it focus on "what" (actions) not "why"?
- **Accuracy:** Are all details correct (no hallucinations)?

### 3. User Optionally Scores

User can provide:
- Their own score (0.0-10.0)
- Feedback/notes on what worked or didn't

*User scoring is optional - sometimes they just share samples without scoring.*

### 4. Documentation

Claude creates:
- Individual sample file: `samples/NNN-description.md`
- Updates summary table: `evaluation-log.md`
- Provides analysis and recommendations

## Scoring Criteria (Detailed)

### Completeness (2.5 points)

**What to check:**
- All critical findings mentioned?
- All action items captured?
- All deliverables (files, docs) noted?
- All next steps or decisions included?

**Scoring:**
- 2.5 = Captured everything important
- 2.0 = Missed one minor item
- 1.5 = Missed one major item or multiple minor items
- 1.0 = Missed multiple major items
- 0.5 = Captured less than half
- 0.0 = Completely missed the point

### Conciseness (2.5 points)

**What to check:**
- Stayed under 8 bullets?
- Each bullet 1-2 sentences max?
- No fluff or unnecessary explanations?
- Focused on key points only?

**Scoring:**
- 2.5 = Perfect - concise and focused
- 2.0 = One bullet could be tighter
- 1.5 = 2-3 bullets too verbose
- 1.0 = Multiple bullets too long
- 0.5 = Overly verbose throughout
- 0.0 = Way too long or unfocused

### Actionability (2.5 points)

**What to check:**
- Focused on "what" not "why"?
- Action-oriented language?
- Clear deliverables noted?
- Decisions and next steps explicit?

**Scoring:**
- 2.5 = Every bullet is actionable
- 2.0 = One bullet could be more action-oriented
- 1.5 = 2-3 bullets lack clarity
- 1.0 = Half the bullets are vague
- 0.5 = Mostly explanations, few actions
- 0.0 = No clear actions

### Accuracy (2.5 points)

**What to check:**
- File paths correct?
- Numbers/metrics accurate?
- No hallucinated details?
- Context preserved correctly?

**Scoring:**
- 2.5 = Completely accurate
- 2.0 = One trivial inaccuracy
- 1.5 = One significant inaccuracy
- 1.0 = Multiple inaccuracies
- 0.5 = Major misrepresentations
- 0.0 = Hallucinated or wrong

## Sample File Format

Each sample in `samples/NNN-description.md`:

```markdown
# Sample NNN: [Brief Description]

**Date:** YYYY-MM-DD
**Type:** Code Implementation | Research | Planning | Debugging | Review
**Context:** [Optional: What task was being done]

## Original Message

[Full original Claude message that was summarized]

## TLDR Output

[The bullet-point summary generated by /tldr]

## Claude's Evaluation

**Overall Score:** X.X / 10.0

### Breakdown
- **Completeness:** X.X / 2.5 - [Reasoning]
- **Conciseness:** X.X / 2.5 - [Reasoning]
- **Actionability:** X.X / 2.5 - [Reasoning]
- **Accuracy:** X.X / 2.5 - [Reasoning]

### Analysis
**What Went Well:**
- [Bullet points]

**What Needs Work:**
- [Bullet points]

### Recommendations
[Specific suggestions for improvement]

## User's Evaluation

**Score:** [X.X / 10.0 or "Not provided"]
**Feedback:** [User notes or "None"]
```

## Summary Log Format

`evaluation-log.md` contains a table:

| ID | Date | Type | Claude Score | User Score | Status | Key Notes |
|----|------|------|--------------|------------|--------|-----------|
| 001 | 2026-02-06 | Code | 8.5 | - | Unscored | Good coverage, missed context |
| 002 | 2026-02-07 | Research | 9.2 | 9.0 | Scored | Excellent summary |

## Message Types

Common categories:
- **Code Implementation** - File changes, functions added, tests
- **Research Summary** - Findings, sources, recommendations
- **Planning Session** - Roadmap, architecture, decisions
- **Debugging/Fixes** - Root cause, solution, verification
- **Review/Analysis** - Audit results, issues found, recommendations
- **Documentation** - Files created, structure, content

## Analysis Phases

### Phase 1: Data Collection (Current)
- Collect 10-20 diverse samples
- Focus on breadth (different message types)
- Identify obvious patterns

### Phase 2: Pattern Analysis
- What types does it handle well?
- What types does it struggle with?
- Common failure modes?
- Edge cases discovered?

### Phase 3: Improvement Planning
- Prioritize fixes for v1.1
- Prompt engineering adjustments
- Consider format options (timeline, actions-only, etc.)

### Phase 4: A/B Testing
- Test prompt variations
- Compare before/after scores
- Validate improvements

## Instructions for Claude

When user says "rate this tldr" or shares a sample:

1. **Read the sample carefully**
   - Understand the original message context
   - Understand what the TLDR captured

2. **Score objectively**
   - Use the detailed criteria above
   - Be honest about weaknesses
   - Justify each score component

3. **Create documentation**
   - Generate next sample number (check evaluation-log.md)
   - Create `samples/NNN-description.md`
   - Update `evaluation-log.md` table
   - Provide immediate feedback in chat

4. **If user provides score/feedback**
   - Add to the sample file
   - Update evaluation-log.md status to "Scored"
   - Note any discrepancies for analysis

5. **Be constructive**
   - Highlight what works well
   - Suggest specific improvements
   - Note patterns across samples

## Success Metrics

**Target for v1.0:**
- Average score: >8.0 / 10.0
- Completeness: >2.0 / 2.5
- User satisfaction: Positive feedback majority

**Improvement Goals:**
- Identify 3-5 clear patterns for v1.1
- Reduce failure modes (scores <6.0)
- Increase consistency across message types

## Future Enhancements

Based on evaluation data, we may add:
- Format options (`/tldr actions`, `/tldr timeline`)
- Message type detection and specialized handling
- Confidence scores (how confident is the summary?)
- Multi-message summarization (`/tldr 3`)
- Export capabilities (`/tldr export`)
